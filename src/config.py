from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Any
from pathlib import Path
import os
import yaml


@dataclass
class InferenceConfig:
    """Inference-related configuration parameters"""

    # Model paths (should be configured in config.yaml)
    strong_model_path: str = "your_strong_model_path"
    weak_model_path: str = "your_weak_model_path"

    # Generation parameters
    max_tokens: int = 2048
    temperature: float = 0.0
    top_p: float = 0.9
    skip_special_tokens: bool = True

    # Server configuration
    base_port: int = 8000
    strong_gpu_ids: List[int] = field(default_factory=lambda: [0, 1])
    weak_gpu_ids: List[int] = field(default_factory=lambda: [2, 3])
    cuda_visible_devices: str = "2,3,4,5"

    # GPT API configuration
    use_azure: bool = False
    openai_api_key: Optional[str] = field(default_factory=lambda: os.environ.get("OPENAI_API_KEY"))
    openai_api_base: Optional[str] = field(default_factory=lambda: os.environ.get("OPENAI_API_BASE", "https://api.ai-gaochao.cn/v1"))
    judge_model: str = "gpt-4o"

    # Parallel inference settings
    max_workers: int = 64
    batch_size: int = 8

    # Template settings
    template_type: str = "default"
    system_prompt: str = "You are a helpful AI assistant."

    # xVerify model configuration for math evaluation
    xverify_model_name: str = "xVerify-9B-C"
    xverify_model_url: str = "http://127.0.0.1:8000/v1"
    xverify_inference_mode: str = "api"
    xverify_api_key: str = "dummy"

    def get_server_urls(self, model_type: str = "weak") -> List[str]:
        """Generate server URLs based on model type"""
        if model_type == "strong":
            gpu_ids = self.strong_gpu_ids
        else:  # weak
            gpu_ids = self.weak_gpu_ids
        return [f"http://localhost:{self.base_port + gpu_id}" for gpu_id in gpu_ids]

    def get_model_path(self, model_type: str = "weak") -> str:
        """Get model path based on type"""
        if model_type == "strong":
            return self.strong_model_path
        else:  # weak
            return self.weak_model_path


@dataclass
class RouterConfig:
    """Router-specific configuration"""
    router_type: str = "probe" # "probe", "self_questioning", "deberta", "trained_deberta", "llm", "logits_margin", "semantic_entropy", "max_logits", "top10_variance", "coe", "entropy", "confidence_margin", "embedding_mlp"
    # Probe router settings
    checkpoint_path: Optional[str] = "probe_save/mixed_mmlu_full_numina_cot_5k_balanced_probe_mean.pt"
    probe_type: str = "mean"
    model_path: Optional[str] = None
    num_samples: int = 5
    embedding_files: Optional[List[str]] = None
    embedding_dir: Optional[str] = None  # optional: directory containing "{task}_query_embeddings.pt"

    def to_dict(self, inference_config=None) -> Dict[str, Any]:
        """Convert to dictionary format for compatibility"""
        config_dict = {"type": self.router_type}

        if self.router_type == "probe":
            config_dict.update({
                "checkpoint_path": self.checkpoint_path,
                "probe_type": self.probe_type,
                "num_samples": self.num_samples
            })
        elif self.router_type == "embedding_mlp":
            config_dict.update({
                "checkpoint_path": self.checkpoint_path,
                "embedding_files": getattr(self, "embedding_files", None),
                "embedding_dir": getattr(self, "embedding_dir", None),
            })
        elif self.router_type in ["self_questioning", "deberta", "trained_deberta", "llm", "logits_margin", "semantic_entropy"]:
            # Use weak model if not specified
            model_path = self.model_path
            if model_path is None and inference_config is not None:
                model_path = inference_config.weak_model_path
            config_dict["model_path"] = model_path

            # Add semantic entropy specific config
            if self.router_type == "semantic_entropy":
                config_dict["num_samples"] = self.num_samples
        elif self.router_type in ["max_logits", "top10_variance", "coe", "entropy", "confidence_margin"]:
            # Logits-based routers don't need additional config
            pass

        return config_dict


@dataclass
class TrainingConfig:
    """Training-related configuration"""

    # Probe training
    epochs: int = 50
    batch_size: int = 32
    learning_rate: float = 1e-4

    # Probe architecture parameters
    mlp_hidden_dims: Optional[List[int]] = None  # Hidden dimensions for MLP probe, e.g., [512, 256]
    mlp_dropout: float = 0.1  # Dropout rate for MLP probe layers
    conv_channels: int = 32  # Number of channels for ConvProbe
    conv_kernel_size: int = 3  # Kernel size for ConvProbe

    logits_output_dir: str = "hs"
    probe_save_path: str = "probe_save"
    query_embedding_output_dir: str = "query_embeddings_output"
    embedding_mlp_save_path: str = "embedding_mlp"
    embedding_hidden_dims: Optional[List[int]] = None
    embedding_dropout: float = 0.1
    seed: int = 42
    # Training-only runtime options (used by train mode; may also be referenced by eval for probe sweeps)
    max_samples: int = 4000
    save_loss_history: bool = False
    probe_types: List[str] = field(default_factory=lambda: ["hs_last_mlp", "mean", "max", "coe_dual_mlp"])

    # DeBERTa router training
    deberta_train_path: Optional[str] = None
    deberta_val_path: Optional[str] = None
    deberta_model_name: str = "microsoft/deberta-v3-base"
    deberta_num_labels: int = 2
    deberta_max_length: int = 512
    deberta_batch_size: int = 16
    deberta_learning_rate: float = 2e-5
    deberta_weight_decay: float = 0.01
    deberta_epochs: int = 3
    deberta_output_dir: str = "deberta_checkpoints"
    
    


@dataclass
class PipelineConfig:
    """Pipeline execution configuration"""

    # Data directories
    data_dir :str ="data"
    output_dir: str = "results/"
    metric_results_dir: str = "metric_results"

    # Evaluation parameters
    recovery_rate_band: Tuple[float, float] = (0.91, 0.92)
    lpm_call_rate_band: Tuple[float, float] = (0.0, 0.1)

    probe_dir: Optional[str] = None  # optional: evaluate all probe checkpoints in a directory
    prepare_steps: List[str] = field(default_factory=lambda: ["scores", "logits", "embeddings"])
    prepare_text_field: str = "instruction"
    prepare_embed_batch_size: int = 64

    # Default datasets
    # default_datasets: List[str] = field(default_factory=lambda: ["aime24"])

    # Sub-configurations
    inference: InferenceConfig = field(default_factory=InferenceConfig)
    router: RouterConfig = field(default_factory=RouterConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)

    @classmethod
    def from_yaml(cls):
        """Load configuration from config.yaml"""
        # Use relative path: config.yaml is located in the project root directory
        config_file = Path(__file__).parent.parent / "config.yaml"

        if not config_file.exists():
            print(f"Config file {config_file} does not exist, using default configuration")
            return cls()

        print(f"Loading configuration from: {config_file}")

        with open(config_file, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)

        # Compatibility handling:
        # - keep prepare_* and probe_dir at PipelineConfig top-level
        # - keep max_samples/save_loss_history/probe_types in TrainingConfig
        training_data = dict(data.get('training', {}) or {})
        router_data = dict(data.get('router', {}) or {})

        lifted_to_pipeline = {}

        # Lift from router -> pipeline
        if "probe_dir" in router_data:
            lifted_to_pipeline["probe_dir"] = router_data.pop("probe_dir")

        # Lift from training -> pipeline (prepare-related)
        for k in ["prepare_steps", "prepare_text_field", "prepare_embed_batch_size"]:
            if k in training_data:
                lifted_to_pipeline[k] = training_data.pop(k)

        # Also accept these keys at top-level and merge into training (training-only options)
        for k in ["max_samples", "save_loss_history", "probe_types"]:
            if k in data and k not in training_data:
                training_data[k] = data[k]

        inference_config = InferenceConfig(**data.get('inference', {}))
        router_config = RouterConfig(**router_data)
        training_config = TrainingConfig(**training_data)

        pipeline_data = {k: v for k, v in data.items()
                        if k not in ['inference', 'router', 'training',
                                     # prevent passing training-only keys twice
                                     'max_samples', 'save_loss_history', 'probe_types']}
        pipeline_data.update(lifted_to_pipeline)

        return cls(
            inference=inference_config,
            router=router_config,
            training=training_config,
            **pipeline_data
        )
