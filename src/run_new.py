from config import PipelineConfig
from pipeline import RouterEvaluationPipeline, get_task_score
from train_router import (
    generate_logits, set_random_seed,
    complete_probe_training_pipeline_with_mixed_datasets,
    train_probe_model
)
import os
config_env = PipelineConfig.from_yaml()
if config_env.inference.cuda_visible_devices:
    os.environ["CUDA_VISIBLE_DEVICES"] = config_env.inference.cuda_visible_devices
import copy
import json
import argparse
import glob
import re
from pathlib import Path
from datetime import datetime


def save_training_history(history, probe_type, task_list, max_samples=None, save_dir="probe_save/loss"):
    """
    Save training loss and accuracy history to a JSON file

    Args:
        history: Dictionary containing train_losses, val_losses, and best_val_loss
        probe_type: Type of probe being trained
        task_list: List of tasks used for training
        max_samples: Maximum number of samples used for training
        save_dir: Directory to save the history file
    """
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)

    # Create filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tasks_str = "_".join(task_list)
    filename = f"{tasks_str}_{probe_type}_{timestamp}.json"
    filepath = save_path / filename

    # Extract training results from history dict
    # The history dict has a 'training_results' key that contains the actual loss data
    training_results = history.get('training_results', {})
    train_losses = training_results.get('train_losses', [])
    val_losses = training_results.get('val_losses', [])
    val_accuracies = training_results.get('val_accuracies', [])
    val_aurocs = training_results.get('val_aurocs', [])
    learning_rates = training_results.get('learning_rates', [])
    best_val_loss = training_results.get('best_val_loss', float('inf'))
    best_val_acc = training_results.get('best_val_acc', 0.0)
    best_val_auroc = training_results.get('best_val_auroc', 0.0)
    initial_lr = training_results.get('initial_lr', 0.0)

    # Save to JSON
    save_data = {
        "probe_type": probe_type,
        "tasks": task_list,
        "datasets": task_list,  # æ·»åŠ æ•°æ®é›†ä¿¡æ¯(ä¸tasksç›¸åŒ)
        "max_samples": max_samples,  # æ·»åŠ ä½¿ç”¨çš„æœ€å¤§æ ·æœ¬æ•°
        "timestamp": timestamp,
        "initial_lr": initial_lr,
        "best_val_loss": best_val_loss,
        "best_val_acc": best_val_acc,
        "best_val_auroc": best_val_auroc,
        "epochs": len(train_losses),
        "train_losses": train_losses,
        "val_losses": val_losses,
        "val_accuracies": val_accuracies,
        "val_aurocs": val_aurocs,
        "learning_rates": learning_rates,
    }

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“Š Training history saved to: {filepath}")
    print(f"   Datasets used: {', '.join(task_list)}")
    if max_samples:
        print(f"   Max samples: {max_samples} ({max_samples/1000:.1f}k)")
    return str(filepath)


def batch_evaluate_probes(base_config, probe_configs, eval_tasks):
    # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
    model_name = _extract_model_name_from_path(base_config.inference.weak_model_path)

    for i, probe_config in enumerate(probe_configs):
        config_copy = copy.deepcopy(base_config)
        config_copy.router.checkpoint_path = probe_config['checkpoint_path']
        config_copy.router.probe_type = probe_config['probe_type']
        config_copy.metric_results_dir = probe_config['metric_results_dir']
        print(f"\n{'='*60}")
        print(f"Running evaluation {i+1}/{len(probe_configs)}")
        print(f"Probe type: {probe_config['probe_type']}")
        print(f"Checkpoint: {probe_config['checkpoint_path']}")
        print(f"{'='*60}")
        pipeline = RouterEvaluationPipeline(config_copy)

        for task in eval_tasks:
            print(f"\nEvaluating task: {task}")

            hidden_states_file = _build_hs_path(task, model_name)

            if not os.path.exists(hidden_states_file):
                print(f"Warning: Hidden states file not found: {hidden_states_file}")
                continue

            datasets = [f"{task}"]

            pipeline.evaluate_complete_pipeline(
                hidden_states_file=hidden_states_file,
                datasets=datasets
            )


def _extract_model_name_from_path(model_path: str) -> str:
    """ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ¨¡å‹åç§°"""
    return os.path.basename(model_path.rstrip('/'))


def _build_hs_path(task: str, model_name: str = None):
    """æ„å»ºhidden statesæ–‡ä»¶è·¯å¾„"""
    if model_name is None:
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å– weak_model_path
        config = PipelineConfig.from_yaml()
        model_name = _extract_model_name_from_path(config.inference.weak_model_path)

    base_dir = os.path.join("..", "hs")
    if task.startswith("mmlu_pro_"):
        base_dir = os.path.join(base_dir, "mmlu_pro")
    return os.path.join(base_dir, f"{model_name}_{task}.pt")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='CoBench Router Evaluation and Training Framework',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument("--mode", type=str, required=True,
                       choices=["get_scores", "get_logits", "train", "eval_probe",
                               "eval_max_k", 
                               "self_based", "logits_based_routers"],
                       help="è¿è¡Œæ¨¡å¼")

    parser.add_argument("--datasets", type=str, nargs="+", default=None,
                       help="è¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨ (ç©ºæ ¼åˆ†éš”)")

    parser.add_argument("--probe_types", type=str, nargs="+",
                       default=["hs_last_mlp", "mean", "max", "coe_dual_mlp"],
                       help="è®­ç»ƒæ¨¡å¼ä¸‹çš„ probe ç±»å‹")

    parser.add_argument("--max_samples", type=int, default=4000,
                       help="è®­ç»ƒæ—¶çš„æœ€å¤§æ ·æœ¬æ•°")

    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")

    parser.add_argument("--save_loss_history", action="store_true",
                       help="åœ¨è®­ç»ƒæ—¶ä¿å­˜ loss å’Œ accuracy å†å²è®°å½•")

    parser.add_argument("--probe_dir", type=str, default=None,
                       help="Probe æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆç”¨äº eval_probe æ¨¡å¼ï¼‰")


    args = parser.parse_args()
    mode = args.mode
    set_random_seed(args.seed)
    config = PipelineConfig().from_yaml()
    pipeline = RouterEvaluationPipeline(config)

    # ==================== æ¨¡å¼: get_scores ====================
    if mode == "get_scores":
        datasets = args.datasets 
        print(f"ğŸ¯ è·å–ä»¥ä¸‹æ•°æ®é›†çš„ scores: {datasets}")
        for task in datasets:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š å¤„ç†ä»»åŠ¡: {task}")
            print(f"{'='*60}")
            try:
                score = get_task_score(config, task=task)
                print(f"âœ… {task} å®Œæˆ")
                if score:
                    print(f"   Score path: {score}")
            except Exception as e:
                print(f"âŒ {task} å¤±è´¥: {e}")

    # ==================== æ¨¡å¼: get_logits ====================
    elif mode == "get_logits":
        datasets = args.datasets 
        print(f"ğŸ¯ ç”Ÿæˆä»¥ä¸‹æ•°æ®é›†çš„ logits: {datasets}")

        for task in datasets:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š å¤„ç†ä»»åŠ¡: {task}")
            print(f"{'='*60}")

            # æ„å»º task_path
            if task.startswith("mmlu_pro_"):
                task_path = os.path.join("./results/mmlu_pro", f"{task}.jsonl")
            else:
                task_path = os.path.join("./results", f"{task}.jsonl")

            if not os.path.exists(task_path):
                print(f"âš ï¸  è­¦å‘Š: ç»“æœæ–‡ä»¶ {task_path} ä¸å­˜åœ¨")
                print(f"   è¯·å…ˆè¿è¡Œ: python run.py --mode get_scores --datasets {task}")
                continue

            try:
                generate_logits(config, task=task, task_path=task_path)
                print(f"âœ… {task} logits ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                print(f"âŒ {task} logits ç”Ÿæˆå¤±è´¥: {e}")

    # ==================== æ¨¡å¼: train ====================
    elif mode == "train":
        datasets = args.datasets 
        probe_types = args.probe_types
        max_samples = args.max_samples
        save_history = args.save_loss_history

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ Probe æ¨¡å‹")
        print(f"ğŸ“Š æ•°æ®é›†: {datasets}")
        print(f"ğŸ”§ Probe ç±»å‹: {probe_types}")
        print(f"ğŸ“ˆ æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
        print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒå†å²: {save_history}")

        for probe_type in probe_types:
            print(f"\n{'='*60}")
            print(f"ğŸ¯ è®­ç»ƒ Probe ç±»å‹: {probe_type}")
            print(f"{'='*60}")

            config.router.probe_type = probe_type

            try:
                # è®­ç»ƒå¹¶è·å–å†å²è®°å½•
                history = complete_probe_training_pipeline_with_mixed_datasets(
                    config,
                    task_list=datasets,
                    max_samples=max_samples,
                    mix_strategy="balanced"
                )

                print(f"âœ… {probe_type} è®­ç»ƒå®Œæˆ")

                # ä¿å­˜è®­ç»ƒå†å²
                if save_history and history:
                    save_training_history(history, probe_type, datasets, max_samples)

            except Exception as e:
                print(f"âŒ {probe_type} è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

    # ==================== æ¨¡å¼: eval_probe ====================
    elif mode == "eval_probe":
        datasets = args.datasets 
        probe_types = args.probe_types
        probe_dir = args.probe_dir
        
        print(f"ğŸ¯ è¯„ä¼° Probe æ¨¡å‹")
        print(f"ğŸ“Š æ•°æ®é›†: {datasets}")
        print(f"ğŸ”§ Probe ç±»å‹: {probe_types}")
        if probe_dir:
            print(f"ğŸ“ Probe ç›®å½•: {probe_dir}")

        # å¦‚æœæŒ‡å®šäº† probe_dirï¼Œä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰ probe æ–‡ä»¶
        if probe_dir:
            probe_files = sorted(glob.glob(f"{probe_dir}/*.pt"))
            print(f"\nåœ¨ {probe_dir} ä¸­æ‰¾åˆ° {len(probe_files)} ä¸ª probe æ–‡ä»¶")

            probe_configs = []
            for pf in probe_files:
                # å°è¯•ä»æ–‡ä»¶åæå– probe_type
                filename = os.path.basename(pf)
                # åŒ¹é…æ¨¡å¼: *_probe_type.pt æˆ– *_train_probe_type.pt
                m = re.search(r'.*?_(?:train_)?([^_]+)\.pt$', filename)
                if m:
                    detected_probe_type = m.group(1)
                    # å¦‚æœæŒ‡å®šäº† probe_typesï¼Œåªå¤„ç†åŒ¹é…çš„ç±»å‹
                    if probe_types and detected_probe_type not in probe_types:
                        continue

                    metric_results_dir = config.metric_results_dir

                    probe_configs.append({
                        "checkpoint_path": pf,
                        "probe_type": detected_probe_type,
                        "metric_results_dir": metric_results_dir,
                    
                    })

            if probe_configs:
                batch_evaluate_probes(config, probe_configs, datasets)
            else:
                print(f"âš ï¸  è­¦å‘Š: åœ¨ {probe_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ probe æ–‡ä»¶")

        # å¦‚æœæ²¡æœ‰æŒ‡å®š probe_dirï¼Œä½¿ç”¨å½“å‰é…ç½®çš„ probe
        else:
            # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
            model_name = _extract_model_name_from_path(config.inference.weak_model_path)

            for probe_type in probe_types:
                print(f"\n{'='*60}")
                print(f" ä½¿ç”¨ Probe ç±»å‹: {probe_type}")
                print(f"{'='*60}")

                config_copy = copy.deepcopy(config)
                config_copy.router.probe_type = probe_type
                # å¦‚æœé…ç½®ä¸­å·²æœ‰ checkpoint_pathï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™éœ€è¦ç”¨æˆ·æŒ‡å®š
                if not config_copy.router.checkpoint_path:
                    print(f"âš ï¸  è­¦å‘Š: æœªæŒ‡å®š checkpoint_pathï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®æˆ–ä½¿ç”¨ --probe_dir")
                    continue

                pipeline_test = RouterEvaluationPipeline(config_copy)

                for task in datasets:
                    print(f"\nğŸ“Š è¯„ä¼°ä»»åŠ¡: {task}")

                    hidden_states_file = _build_hs_path(task,model_name)

                    if not os.path.exists(hidden_states_file):
                        print(f"âš ï¸  è­¦å‘Š: Hidden states æ–‡ä»¶ä¸å­˜åœ¨: {hidden_states_file}")
                        continue

                    try:
                        results = pipeline_test.evaluate_complete_pipeline(
                            hidden_states_file=hidden_states_file,
                            datasets=[task]
                        )
                        print(f"âœ… {task} ä½¿ç”¨ {probe_type} è¯„ä¼°å®Œæˆ")
                    except Exception as e:
                        print(f"âŒ {task} ä½¿ç”¨ {probe_type} è¯„ä¼°å¤±è´¥: {e}")

    
    elif mode == "self_based":
        # ä»é…ç½®ä¸­è·å–æ¨¡å‹åç§°
        model_name = _extract_model_name_from_path(config.inference.weak_model_path)

        strategies = [
            {
                "name": "semantic_entropy",
                "metric_results_dir": "metric_results/base/semantic_entropy",
                "num_samples": 5,
            },
            {
                "name": "self_questioning",
                "metric_results_dir": "metric_results/base/self_questioning",
                "num_samples": 8,
            },
        ]

        eval_datasets = args.datasets

        for strat in strategies:
            print(f"\n{'='*60}")
            print(f"ğŸš€ è¿è¡Œ self-based ç­–ç•¥: {strat['name']}")
            print(f"{'='*60}")

            config_copy = copy.deepcopy(config)
            config_copy.metric_results_dir = strat["metric_results_dir"]
            config_copy.router.router_type = strat["name"]
            config_copy.router.model_path = None
            config_copy.router.num_samples = strat["num_samples"]

            pipeline_test = RouterEvaluationPipeline(config_copy)

            for task in eval_datasets:
                print(f"\nè¯„ä¼°ä»»åŠ¡: {task}")
                hidden_states_file = _build_hs_path(task, model_name)

                if not os.path.exists(hidden_states_file):
                    print(f"è­¦å‘Š: Hidden states æ–‡ä»¶ä¸å­˜åœ¨: {hidden_states_file}")
                    continue

                datasets = [task]
                pipeline_test.evaluate_complete_pipeline(
                    hidden_states_file=hidden_states_file,
                    datasets=datasets
                )
    elif mode == "logits_based_routers":
        from router import RouterManager

        router_manager = RouterManager()
        router_manager.create_max_logits_router()
        router_manager.create_top10_variance_router()
        router_manager.create_coe_router()
        router_manager.create_entropy_router()
        router_manager.create_confidence_margin_router()

        router_types = ["max_logits", "top10_variance", "coe", "entropy", "confidence_margin"]
        eval_datasets = args.datasets 
        for router_type in router_types:
            print(f"\n{'='*60}")
            print(f"ğŸš€ æµ‹è¯•è·¯ç”±å™¨: {router_type}")
            print(f"{'='*60}")

            for task in eval_datasets:
                print(f"\nè¯„ä¼°ä»»åŠ¡: {task}")

                hidden_states_file = _build_hs_path(task)

                if not os.path.exists(hidden_states_file):
                    print(f"è­¦å‘Š: Hidden states æ–‡ä»¶ä¸å­˜åœ¨: {hidden_states_file}")
                    continue

                config_copy = copy.deepcopy(config)
                config_copy.router.router_type = router_type
                config_copy.metric_results_dir = f"metric_results/base/{router_type}"

                pipeline_test = RouterEvaluationPipeline(config_copy)

                datasets = [task]
                pipeline_test.evaluate_complete_pipeline(
                    hidden_states_file=hidden_states_file,
                    datasets=datasets
                )

    else:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {mode}")
        parser.print_help()

