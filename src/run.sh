#!/bin/bash

# æ¿€æ´»condaç¯å¢ƒ
source /volume/pt-train/users/wzhang/ghchen/zh/miniconda3/bin/activate router

# é»˜è®¤å‚æ•°
DATASETS="${1:-mmlu_test}"
PROBE_TYPES="${2:-hs_last_mlp mean max coe_dual_mlp}"
MAX_SAMPLES="${3:-10000}"

echo "========================================="
echo "CoBench å®Œæ•´ Pipeline"
echo "========================================="
echo "æ•°æ®é›†: $DATASETS"
echo "Probe ç±»å‹: $PROBE_TYPES"
echo "æœ€å¤§æ ·æœ¬æ•°: $MAX_SAMPLES"

# cd inference
# python start.py \
#   --model_path "/volume/pt-train/models/Llama-3.1-8B-Instruct" \
#   --base_port 8000 \
#   --gpu_list "0,1,2,3"



# CUDA_VISIBLE_DEVICES=4 \
# vllm serve /volume/pt-train/users/wzhang/ghchen/zh/models/xVerify-9B-C \
#   --host 0.0.0.0 \
#   --port 8000 \
#   --tensor-parallel-size 1 \
#   --served-model-name xVerify-0.5B \
#   --trust-remote-code

# scores
# python run_new.py --mode get_scores --datasets $DATASETS
# # logits
# python run_new.py --mode get_logits --datasets $DATASETS
# # training probe
python run_new.py --mode train --datasets $DATASETS --probe_types $PROBE_TYPES --max_samples $MAX_SAMPLES --save_loss_history


# # è¯„ä¼°
# python run.py --mode eval_probe --datasets $DATASETS --probe_types $PROBE_TYPES

echo ""
echo "========================================="
echo "ğŸ‰ å®Œæ•´ Pipeline æ‰§è¡ŒæˆåŠŸï¼"
echo "========================================="
echo "ç»“æœä¿å­˜ä½ç½®:"
echo "  - Scores:    results/"
echo "  - Logits:    ../hs/"
echo "  - æ¨¡å‹:      probe_save/test/"
echo "  - è®­ç»ƒå†å²:   probe_save/loss/"
echo "  - è¯„ä¼°ç»“æœ:   metric_results/eval/"
