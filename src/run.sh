#!/bin/bash

# æ¿€æ´»condaç¯å¢ƒ
source /volume/pt-train/users/wzhang/ghchen/zh/miniconda3/bin/activate router

# é»˜è®¤å‚æ•°
DATASETS="${1:- mmlu_train big_math_5k_train}"
PROBE_TYPES="${2:-dynamic_dirichlet}"
MAX_SAMPLES="${3:-8000}"


# MMLU Pro æµ‹è¯•æ•°æ®é›†
MMLU_PRO_TASKS="mmlu_pro_biology mmlu_pro_business mmlu_pro_chemistry mmlu_pro_computer_science mmlu_pro_economics mmlu_pro_engineering mmlu_pro_health mmlu_pro_history mmlu_pro_law mmlu_pro_math mmlu_pro_other mmlu_pro_philosophy mmlu_pro_physics mmlu_pro_psychology"

# å®Œæ•´æµ‹è¯•æ•°æ®é›†åˆ—è¡¨
TEST_DATASETS="${4:-math mmlu_pro_biology mmlu_pro_business mmlu_pro_chemistry mmlu_pro_computer_science mmlu_pro_economics mmlu_pro_engineering mmlu_pro_health mmlu_pro_history mmlu_pro_law mmlu_pro_math mmlu_pro_other mmlu_pro_philosophy mmlu_pro_physics mmlu_pro_psychology magpie_5k_test alpaca_5k_test big_math_5k_test mmlu_test}"
echo "========================================="
echo "CoBench å®Œæ•´ Pipeline"
echo "========================================="
echo "è®­ç»ƒæ•°æ®é›†: $DATASETS"
echo "æµ‹è¯•æ•°æ®é›†: $TEST_DATASETS"
echo "Probe ç±»å‹: $PROBE_TYPES"
echo "æœ€å¤§æ ·æœ¬æ•°: $MAX_SAMPLES"

# ========================================
# æµ‹è¯•æ•°æ®çš„æ­¥éª¤
# ========================================
# å¯åŠ¨æ¨¡å‹æœåŠ¡
# cd inference
#  python start.py \
#   --model_path "/mnt/yixiali/MODELS/meta-llama/Llama-3.1-8B-Instruct" \
#   --base_port 8001 \
#   --gpu_list "1"



# å¦‚æœæµ‹è¯•égeneralæ•°æ® éœ€è¦å¯åŠ¨xVerify
# CUDA_VISIBLE_DEVICES=7 \
# vllm serve /volume/pt-train/users/wzhang/ghchen/zh/models/xVerify-9B-C \
#   --host 0.0.0.0 \
#   --port 8002 \
#   --tensor-parallel-size 1 \
#   --served-model-name xVerify \
#   --trust-remote-code

# ç­‰å¾…æ¨¡å‹æœåŠ¡å¯åŠ¨å®Œæˆåï¼Œè¿è¡Œ scores
# scores
python run_new.py --mode get_scores --datasets $DATASETS
# # # logits
# python run_new.py --mode get_logits --datasets $DATASETS
# # training probe
# python run_new.py --mode train --datasets $DATASETS --probe_types $PROBE_TYPES --max_samples $MAX_SAMPLES --save_loss_history


# # è¯„ä¼°
# python run_new.py --mode eval_probe --datasets $TEST_DATASETS --probe_types $PROBE_TYPES
# python run_new.py --mode logits_based_routers --datasets $TEST_DATASETS 
# python run_new.py --mode self_based --datasets $TEST_DATASETS 
echo ""
echo "========================================="
echo "ğŸ‰ å®Œæ•´ Pipeline æ‰§è¡ŒæˆåŠŸï¼"
echo "========================================="
echo "ç»“æœä¿å­˜ä½ç½®:"
echo "  - Scores:    results/"
echo "  - Logits:    ../hs/"
echo "  - æ¨¡å‹:      probe_save/test/"
echo "  - è®­ç»ƒå†å²:   probe_save/loss/"
echo "  - è¯„ä¼°ç»“æœ:   metric_results/eval/"


# ========================================
# å¯åŠ¨æ¨¡å‹æœåŠ¡
# cd inference
# ts --gpu_indices 0,1,2,3 python start.py \
#   --model_path "/mnt/yixiali/MODELS/meta-llama/Llama-3.1-8B-Instruct" \
#   --base_port 8001 \
#   --gpu_list "0,1,2,3"


# ts -G 1 vllm serve IAAR-Shanghai/xVerify-9B-C \
#   --host 0.0.0.0 \
#   --port 8000 \
#   --tensor-parallel-size 1 \
#   --served-model-name xVerify \
#   --trust-remote-code



# conda activate;cd src
# ts bash run.sh alpaca_10k
# ts bash run.sh big_math_10k