{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set, test set, and generalization set split\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "data_path = './processed_outputs/output_data_filtered_judge_with_human_label.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_path2 = './processed_outputs/output_data_filtered2_judge_with_human_label.json'\n",
    "with open(data_path2, 'r') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "all_data = data + data2\n",
    "\n",
    "train_test_dataset = ['MMLU-Pro_enh', 'MMLU-Pro', 'DROP', 'GPQA_enh', 'GPQA', 'C-SimpleQA', 'FRAMES', 'AIME_2024', 'AMC23', 'OlympiadBench_en', 'OlympiadBench_zh', 'MGSM', 'CMATH', 'GSM8K', 'MATH', 'AgNews', 'CHID', 'CMMLU_enh', 'CMMLU', 'MMLU_enh', 'MMLU', 'CLUEWSC']\n",
    "general_dataset = ['MMLU-Redux_enh', 'MMLU-Redux', 'C-Eval_enh', 'C-Eval', 'SimpleQA', 'ARC', 'LiveMathBench_zh', 'LiveMathBench_en', 'Amazon', 'CMNLI']\n",
    "\n",
    "\n",
    "train_test_data = []\n",
    "general_data = []\n",
    "\n",
    "for item in all_data:\n",
    "    if item[\"GPT_4o_judgment_consistency\"] != \"True\":\n",
    "        continue\n",
    "    if item[\"dataset\"] in train_test_dataset:\n",
    "        train_test_data.append(item)\n",
    "    else:\n",
    "        general_data.append(item)\n",
    "\n",
    "print(len(all_data), len(train_test_data), len(general_data))\n",
    "\n",
    "random.shuffle(train_test_data)\n",
    "train = train_test_data[:35569]\n",
    "test = train_test_data[35569:]\n",
    "\n",
    "random.shuffle(general_data)\n",
    "general = general_data[:4000]\n",
    "\n",
    "print(len(train), len(test), len(general))\n",
    "\n",
    "save_dir = './processed_outputs/experiment_dataset'\n",
    "with open(os.path.join(save_dir, 'train_raw.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(train, f, ensure_ascii=False, indent=4)\n",
    "with open(os.path.join(save_dir, 'test_raw.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(test, f, ensure_ascii=False, indent=4)\n",
    "with open(os.path.join(save_dir, 'general_raw.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(general, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge manually annotated data into the test set\n",
    "import json\n",
    "\n",
    "\n",
    "data_path = './processed_outputs/experiment_dataset/test_raw.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_path2 = './processed_outputs/test_raw_human_labeled.json'\n",
    "with open(data_path2, 'r') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "print(len(data), len(data2))\n",
    "\n",
    "num = len(data)\n",
    "not_consistency_num = 0\n",
    "cnt = 0\n",
    "for item in data:\n",
    "    if item['GPT_4o_judgment_consistency'] == \"False\" or item['key_answer_type'] == \"math\":\n",
    "        continue\n",
    "    assert item['question'] == data2[cnt]['question']\n",
    "    assert item['model_name'] == data2[cnt]['model_name']\n",
    "    assert data2[cnt].keys() - item.keys() == {'human_judgment_result'}\n",
    "    if item['GPT_4o_final_judgment_result'] != data2[cnt]['human_judgment_result']:\n",
    "        not_consistency_num += 1\n",
    "    item['human_judgment_result'] = data2[cnt]['human_judgment_result']\n",
    "    cnt += 1\n",
    "\n",
    "print(cnt, not_consistency_num)\n",
    "\n",
    "output_path = './processed_outputs/experiment_dataset/test_raw2.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge manually annotated data into the generalization set\n",
    "import json\n",
    "\n",
    "\n",
    "data_path = './processed_outputs/experiment_dataset/general_raw.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data_path2 = './processed_outputs/general_raw_human_labeled.json'\n",
    "with open(data_path2, 'r') as f:\n",
    "    data2 = json.load(f)\n",
    "\n",
    "print(len(data), len(data2))\n",
    "\n",
    "num = len(data)\n",
    "not_consistency_num = 0\n",
    "cnt = 0\n",
    "for item in data:\n",
    "    if item['GPT_4o_judgment_consistency'] == \"False\" or item['key_answer_type'] == \"math\":\n",
    "        continue\n",
    "    assert item['question'] == data2[cnt]['question']\n",
    "    assert item['model_name'] == data2[cnt]['model_name']\n",
    "    assert data2[cnt].keys() - item.keys() == {'human_judgment_result'}\n",
    "    if item['GPT_4o_final_judgment_result'] != data2[cnt]['human_judgment_result']:\n",
    "        not_consistency_num += 1\n",
    "    item['human_judgment_result'] = data2[cnt]['human_judgment_result']\n",
    "    cnt += 1\n",
    "\n",
    "print(cnt, not_consistency_num)\n",
    "\n",
    "output_path = './processed_outputs/experiment_dataset/general_raw2.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical information of each dataset\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# train: ./processed_outputs/experiment_dataset/train_raw.json\n",
    "# test: ./processed_outputs/experiment_dataset/test_raw2.json\n",
    "# general: ./processed_outputs/experiment_dataset/general_raw2.json\n",
    "data_path = './processed_outputs/experiment_dataset/test_raw2.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "models = []\n",
    "datasets = []\n",
    "prompt_types = []\n",
    "key_answer_types = []\n",
    "model_output_length = []\n",
    "judgments = []\n",
    "for item in data:\n",
    "    models.append(item['model_name'])\n",
    "    datasets.append(item['dataset'])\n",
    "    prompt_types.append(item['setting'])\n",
    "    key_answer_types.append(item['key_answer_type'])\n",
    "    model_output_length.append(len(item['llm_output']))\n",
    "    if 'train' in data_path:\n",
    "        if 'human_judgment_result' not in item:\n",
    "            judgments.append(item['GPT_4o_final_judgment_result'])\n",
    "        else:\n",
    "            judgments.append(item['human_judgment_result'])\n",
    "    else:\n",
    "        judgments.append(item['human_judgment_result'])\n",
    "\n",
    "print(json.dumps(Counter(models), indent=4))\n",
    "print(json.dumps(Counter(datasets), indent=4))\n",
    "print(json.dumps(Counter(prompt_types), indent=4))\n",
    "print(json.dumps(Counter(key_answer_types), indent=4))\n",
    "print(json.dumps(Counter(judgments), indent=4))\n",
    "\n",
    "print(\n",
    "    \" length < 1000: \", len([i for i in model_output_length if i < 1000]), '\\n',\n",
    "    \"1000 <= length < 2000: \", len([i for i in model_output_length if 1000 <= i < 2000]), '\\n',\n",
    "    \"2000 <= length < 3000: \", len([i for i in model_output_length if 2000 <= i < 3000]), '\\n',\n",
    "    \"3000 <= length < 4000: \", len([i for i in model_output_length if 3000 <= i < 4000]), '\\n',\n",
    "    \"4000 <= length < 5000: \", len([i for i in model_output_length if 4000 <= i < 5000]), '\\n',\n",
    "    \"5000 <= length < 6000: \", len([i for i in model_output_length if 5000 <= i < 6000]), '\\n',\n",
    "    \"6000 <= length: \", len([i for i in model_output_length if 6000 <= i]), '\\n',\n",
    "    \"avg length:\", round(sum(model_output_length) / len(model_output_length), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset that needs augmentation and perform data augmentation\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Sequentially switch to the training set, test set, and generalization set\n",
    "data_path = \"./processed_outputs/experiment_dataset/train_raw2.json\"\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Sequentially check the target sample sizes for each type of data augmentation\n",
    "    \n",
    "# Replace the final answer sentence pattern\n",
    "sample_data = []\n",
    "for item in data:\n",
    "    if item['key_answer_type'] == \"alphabet_option\" and item['setting'].endswith('restrict'):\n",
    "        sample_data.append(item)\n",
    "print(len(sample_data))\n",
    "\n",
    "# Equivalent substitution of the correct answer\n",
    "answer_replace_data = []\n",
    "for item in data:\n",
    "    if item['key_answer_type'] == 'math' and 4 < len(str(item['correct_answer'])) <= 50 and not bool(re.search(r'[\\u4e00-\\u9fff]', str(item['correct_answer']))):\n",
    "        answer_replace_data.append(item)\n",
    "print(len(answer_replace_data))\n",
    "\n",
    "# Equivalent substitution of key response phrases\n",
    "pattern = r'The answer is (.+?)\\.'\n",
    "output_replace_data = []\n",
    "for item in data:\n",
    "    if item['key_answer_type'] == 'math' and item['setting'].endswith('restrict'):\n",
    "        match = re.search(pattern, item[\"llm_output\"])\n",
    "        if match and match.span()[1] == len(item[\"llm_output\"]) and 4 < len(match.group(1)) <= 50 and any(char.isdigit() for char in match.group(1)):\n",
    "            output_replace_data.append(item)\n",
    "print(len(output_replace_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the final answer sentence pattern\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "sample_data = []\n",
    "for item in data:\n",
    "    if item['key_answer_type'] == \"alphabet_option\" and item['setting'].endswith('restrict'):\n",
    "        sample_data.append(item)\n",
    "\n",
    "random.shuffle(sample_data)\n",
    "\n",
    "sample_ratio = 0.6  # Sampling ratio\n",
    "sample_num = int(len(sample_data) * sample_ratio)\n",
    "sample_data2 = sample_data[:sample_num]\n",
    "\n",
    "# Template for the final answer statement to be replaced\n",
    "final_answer_prompts = [\n",
    "    \"the most appropriate answer is {final_answer}\",\n",
    "    \"the most logical answer is {final_answer}\",\n",
    "    \"the most fitting answer based on the information given is {final_answer}\",\n",
    "    \"I would choose {final_answer}\",\n",
    "    \"I would select {final_answer}\",\n",
    "    \"the most suitable answer is {final_answer}\",\n",
    "    \"the most reasonable answer is {final_answer}\",\n",
    "    \"the correct answer would be {final_answer}\",\n",
    "    \"the closest answer is {final_answer}\",\n",
    "    \"the most accurate answer is {final_answer}\",\n",
    "    \"the answer should be {final_answer}\",\n",
    "    \"the best answer is {final_answer}\",\n",
    "    \"I choose {final_answer}\",\n",
    "    \"the most likely answer is {final_answer}\",\n",
    "    \"the option that best fits this scenario is {final_answer}\",\n",
    "    \"the answer {final_answer} is the most appropriate\",\n",
    "    \"the answer {final_answer} is the most logical\",\n",
    "    \"the answer {final_answer} is the most fitting\",\n",
    "    \"the answer {final_answer} is correct\",\n",
    "    \"{final_answer} seems to be the most promising\"\n",
    "]\n",
    "\n",
    "wrap_type = [\"({final_answer})\", \"[{final_answer}]\", \"<{final_answer}>\", \"'{final_answer}'\",\n",
    "             \"({final_answer}\", \"[{final_answer}\", \"<{final_answer}\", \"'{final_answer}\",\n",
    "             \"{final_answer})\", \"{final_answer}]\", \"{final_answer}>\", \"{final_answer}'\", \"{final_answer}\",\n",
    "             \"（{final_answer}）\", \"【{final_answer}】\", \"《{final_answer}》\", \"‘{final_answer}’\", \"\\\\boxed{{{final_answer}}}\"]\n",
    "gap_type = [\"\\\\ {final_answer}\", \": {final_answer} \", \": {final_answer}\", \"# {final_answer} \", \"{final_answer}\"]\n",
    "pattern = r'The answer is (.+?)\\.'\n",
    "\n",
    "new_data = []\n",
    "for item in sample_data2:\n",
    "    # match = re.search(pattern, item[\"llm_output\"])\n",
    "    matches = list(re.finditer(pattern, item[\"llm_output\"]))\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1)\n",
    "        final_answer = final_answer.replace(\"(\", \"\").replace(\")\", \"\").replace(\" \", \"\")\n",
    "        # random select wrap type\n",
    "        wrap_type_idx = 0\n",
    "        wrap_type_idx = random.randint(0, len(wrap_type) - 1)\n",
    "        final_answer = wrap_type[wrap_type_idx].format(final_answer=final_answer)\n",
    "        # random select gap type\n",
    "        gap_type_idx = 0\n",
    "        gap_type_idx = random.randint(0, len(gap_type) - 1)\n",
    "        final_answer = gap_type[gap_type_idx].format(final_answer=final_answer)\n",
    "        # random select final answer prompt\n",
    "        final_answer_prompt_idx = 0\n",
    "        final_answer_prompt_idx = random.randint(0, len(final_answer_prompts) - 1)\n",
    "        final_answer_prompt = final_answer_prompts[final_answer_prompt_idx].format(final_answer=final_answer)\n",
    "        try:\n",
    "            new_text = re.sub(pattern, final_answer_prompt, item[\"llm_output\"])\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        item[\"llm_output\"] = new_text\n",
    "        new_data.append(item)\n",
    "\n",
    "print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent substitution of the correct answer\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.llms import LLMs\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "prompt_path = './prompts/generate_answer_en.txt'\n",
    "with open(prompt_path, 'r') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "model = LLMs('GPT_4o')\n",
    "\n",
    "answer_replace_data = []\n",
    "for item in data:\n",
    "    if item['key_answer_type'] == 'math' and 4 < len(str(item['correct_answer'])) <= 50 and not bool(re.search(r'[\\u4e00-\\u9fff]', str(item['correct_answer']))):\n",
    "        answer_replace_data.append(item)\n",
    "\n",
    "random.shuffle(answer_replace_data)\n",
    "\n",
    "new_data1 = []\n",
    "sample_ratio = 0.13\n",
    "sample_num = int(len(answer_replace_data) * sample_ratio)\n",
    "answer_replace_data = answer_replace_data[:sample_num]\n",
    "with tqdm(total=sample_num, desc=\"Processing\", unit=\"task\") as pbar:\n",
    "    for item in answer_replace_data:\n",
    "        input = prompt.format(\n",
    "            question=item['question'],\n",
    "            answer=item['correct_answer']\n",
    "        )\n",
    "        response = model.request(input)\n",
    "        response = response[response.find('```json')+7:response.rfind('```')]\n",
    "        response = eval(response)\n",
    "\n",
    "        for k, v in response.items():\n",
    "            new_item = item.copy()\n",
    "            new_item['correct_answer'] = v\n",
    "            new_data1.append(new_item)\n",
    "        pbar.update(1)\n",
    "\n",
    "print(len(new_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent substitution of key response phrases\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.llms import LLMs\n",
    "\n",
    "\n",
    "random.seed(52)\n",
    "\n",
    "\n",
    "prompt_path = './prompts/generate_output_en.txt'\n",
    "with open(prompt_path, 'r') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "model = LLMs('GPT_4o')\n",
    "\n",
    "pattern = r'The answer is (.+?)\\.'\n",
    "output_replace_data = []\n",
    "for item in data:\n",
    "    if item['key_answer_type'] == 'math' and item['setting'].endswith('restrict'):\n",
    "        match = re.search(pattern, item[\"llm_output\"])\n",
    "        if match and match.span()[1] == len(item[\"llm_output\"]) and 4 < len(match.group(1)) <= 50 and any(char.isdigit() for char in match.group(1)):\n",
    "            output_replace_data.append(item)\n",
    "\n",
    "random.shuffle(output_replace_data)\n",
    "\n",
    "new_data2 = []\n",
    "sample_ratio = 0.47\n",
    "sample_num = int(len(output_replace_data) * sample_ratio)\n",
    "output_replace_data = output_replace_data[:sample_num]\n",
    "with tqdm(total=sample_num, desc=\"Processing\", unit=\"task\") as pbar:\n",
    "    for item in output_replace_data:\n",
    "        match = re.search(pattern, item[\"llm_output\"])\n",
    "        \n",
    "        input = prompt.format(\n",
    "            output=match.group(1)\n",
    "        )\n",
    "        response = model.request(input)\n",
    "        response = response[response.find('```json')+7:response.rfind('```')]\n",
    "        response = eval(response)\n",
    "\n",
    "        for k, v in response.items():\n",
    "            # print(v)\n",
    "            new_item = item.copy()\n",
    "            new_item['llm_output'] = new_item['llm_output'][:match.span()[0]] + v\n",
    "            new_data2.append(new_item)\n",
    "        pbar.update(1)\n",
    "\n",
    "print(len(new_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate original data and augmented data\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "print(len(data), len(new_data), len(new_data1), len(new_data2))\n",
    "all_data = data + new_data + new_data1 + new_data2\n",
    "\n",
    "\n",
    "data_path_ = pathlib.Path(data_path)\n",
    "raw_name = data_path_.stem\n",
    "save_path = os.path.join(data_path_.parent, raw_name + '_enh.json')\n",
    "with open(save_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(len(all_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a fine-tuning training set in Alpaca format\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "\n",
    "data_path = \"./processed_outputs/experiment_dataset/train_raw_enh.json\"\n",
    "with open(data_path, 'r') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "prompt_path = './prompts/xverify_prompt.yaml'\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_data = yaml.safe_load(file)\n",
    "\n",
    "train_sft = []\n",
    "for item in train:\n",
    "    input = prompt_data['prompt'].format(\n",
    "        question=item['question'],\n",
    "        output=item['llm_output'],\n",
    "        answer=item['correct_answer']\n",
    "    )\n",
    "\n",
    "    train_sft.append({\n",
    "                \"instruction\": input,\n",
    "                \"input\": \"\",\n",
    "                \"output\": item['human_judgment_result'] if 'human_judgment_result' in item else item['GPT_4o_final_judgment_result']\n",
    "            })\n",
    "\n",
    "save_dir = './processed_outputs/experiment_dataset'\n",
    "with open(os.path.join(save_dir, 'train_raw_enh_formatter_as_alpaca.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_sft, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
