{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate statistics of all results\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "llm_output_dir = './llm_outputs/'\n",
    "output_num = 0\n",
    "file_num = 0\n",
    "prompt_types = ['0_shot_cot_restrict', '0_shot_cot', '0_shot_restrict', '0_shot', '5_shot_cot_restrict', '5_shot_cot', '5_shot_restrict', '5_shot']\n",
    "prompt_type_file_count = {i: 0 for i in prompt_types}\n",
    "prompt_type_count = {i: 0 for i in prompt_types}\n",
    "models = ['GPT_4o', 'LLaMA3.1-8B', 'DeepSeek-R1-Distill-Llama-8B', 'DeepSeek-R1-Distill-Qwen-1.5B', 'DeepSeek-R1-Distill-Qwen-7B', 'DeepSeek-R1-Distill-Qwen-14B', 'Qwen2-7B-Instruct', 'Qwen2-1.5B-Instruct', 'Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'InternLM2.5-7B-Chat', 'ChatGLM3-6B', 'GLM-4-9B-Chat', 'Gemma-2-9B-it', 'Gemma-2-2B-it', \"Llama-3.2-1B-Instruct\", \"Llama-3.2-3B-Instruct\", \"QwQ-32B-2\", \"Phi-4\"]\n",
    "model_file_count = {i: 0 for i in models}\n",
    "model_count = {i: 0 for i in models}\n",
    "model_output_avg_length = {i: [0, 0] for i in models}\n",
    "datasets = ['MMLU-Redux_enh', 'MMLU-Redux', 'MMLU-Pro_enh', 'MMLU-Pro', 'DROP', 'GPQA_enh', 'GPQA', 'C-SimpleQA', 'FRAMES', 'AIME_2024', 'LiveMathBench_zh', 'LiveMathBench_en', 'CLUEWSC', 'C-Eval_enh', 'C-Eval', 'SimpleQA', 'AMC23', 'OlympiadBench_en', 'OlympiadBench_zh', 'MGSM', 'CMATH', 'GSM8K', 'MATH', 'AgNews', 'Amazon', 'CMNLI', 'ARC', 'CHID', 'CMMLU_enh', 'CMMLU', 'MMLU_enh', 'MMLU']\n",
    "dataset_file_count = {i: 0 for i in datasets}\n",
    "dataset_count = {i: 0 for i in datasets}\n",
    "key_answer_type_count = {\n",
    "    \"alphabet_option\": 0,\n",
    "    \"math\": 0,\n",
    "    \"short_text\": 0,\n",
    "    \"categorical_label\": 0\n",
    "}\n",
    "output_len_ls = []\n",
    "for file in os.listdir(llm_output_dir):\n",
    "    if not file.endswith('json'):\n",
    "        continue\n",
    "    file_path = os.path.join(llm_output_dir, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data_num = len(data['results'])\n",
    "    data_output_len_ls = []\n",
    "    for item in data['results']:\n",
    "        data_output_len_ls.append(len(item['llm_output']))\n",
    "        key_answer_type_count[item[\"key_answer_type\"]] += 1\n",
    "    output_num += len(data['results'])\n",
    "    for prompt_type in prompt_type_file_count:\n",
    "        if file.startswith(prompt_type):\n",
    "            prompt_type_file_count[prompt_type] += 1\n",
    "            prompt_type_count[prompt_type] += data_num\n",
    "            break\n",
    "    for model in model_file_count:\n",
    "        if file.__contains__(model):\n",
    "            model_file_count[model] += 1\n",
    "            model_count[model] += data_num\n",
    "            model_output_avg_length[model][0] += len(data['results'])\n",
    "            model_output_avg_length[model][1] += sum(data_output_len_ls)\n",
    "            break\n",
    "    for dataset in dataset_file_count:\n",
    "        if file.__contains__(dataset):\n",
    "            dataset_file_count[dataset] += 1\n",
    "            dataset_count[dataset] += data_num\n",
    "            break\n",
    "    file_num += 1\n",
    "    output_len_ls.extend(data_output_len_ls)\n",
    "\n",
    "for model in models:\n",
    "    model_output_avg_length[model] = model_output_avg_length[model][1] / model_output_avg_length[model][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num, output_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_answer_type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_count, model_file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_count, dataset_file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type_count, prompt_type_file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \" length < 1000: \", len([i for i in output_len_ls if i < 1000]), '\\n',\n",
    "    \"1000 <= length < 2000: \", len([i for i in output_len_ls if 1000 <= i < 2000]), '\\n',\n",
    "    \"2000 <= length < 3000: \", len([i for i in output_len_ls if 2000 <= i < 3000]), '\\n',\n",
    "    \"3000 <= length < 4000: \", len([i for i in output_len_ls if 3000 <= i < 4000]), '\\n',\n",
    "    \"4000 <= length < 5000: \", len([i for i in output_len_ls if 4000 <= i < 5000]), '\\n',\n",
    "    \"5000 <= length < 6000: \", len([i for i in output_len_ls if 5000 <= i < 6000]), '\\n',\n",
    "    \"6000 <= length: \", len([i for i in output_len_ls if 6000 <= i]), '\\n',\n",
    "    \"avg length:\", round(sum(output_len_ls) / len(output_len_ls), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the outcomes from all large models' responses\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "llm_output_dir = './llm_outputs/'\n",
    "output_all = []\n",
    "for file in os.listdir(llm_output_dir):\n",
    "    if not file.endswith('json'):\n",
    "        continue\n",
    "    file_path = os.path.join(llm_output_dir, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    setting = data['info']['setting']\n",
    "    model_name = data['info']['llm']['model_name']\n",
    "    for item in data['results']:\n",
    "        item_ = item.copy()\n",
    "        del item_['prompt']\n",
    "        del item_['index']\n",
    "        output_all.append(\n",
    "            {\n",
    "                \"setting\": setting,\n",
    "                \"model_name\": model_name,\n",
    "                **item_\n",
    "            }\n",
    "        )\n",
    "\n",
    "with open('./processed_outputs/output_all.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_all, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter samples\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "data_path = './processed_outputs/output_all.json'\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "alphabet_data = []\n",
    "math_data = []\n",
    "short_text_data = []\n",
    "categorical_data = []\n",
    "\n",
    "for item in data:\n",
    "    if item[\"key_answer_type\"] == \"alphabet_option\":\n",
    "        alphabet_data.append(item)\n",
    "    elif item[\"key_answer_type\"] == \"math\":\n",
    "        math_data.append(item)\n",
    "    elif item[\"key_answer_type\"] == \"short_text\":\n",
    "        short_text_data.append(item)\n",
    "    elif item[\"key_answer_type\"] == \"categorical_label\":\n",
    "        categorical_data.append(item)\n",
    "\n",
    "random.shuffle(alphabet_data)\n",
    "random.shuffle(math_data)\n",
    "random.shuffle(short_text_data)\n",
    "# random.shuffle(categorical_data)\n",
    "\n",
    "# print(len(alphabet_data), len(math_data), len(short_text_data), len(categorical_data))\n",
    "\n",
    "output_data_filtered = alphabet_data[:8000] + \\\n",
    "                       math_data[:8000] + \\\n",
    "                       short_text_data[:8000] + \\\n",
    "                       categorical_data[:8000]\n",
    "\n",
    "output_data_remaining1 = alphabet_data[8000:] + \\\n",
    "                         math_data[8000:] + \\\n",
    "                         short_text_data[8000:] + \\\n",
    "                         categorical_data[:8000]\n",
    "\n",
    "\n",
    "output_data_remaining2 = []\n",
    "for item in output_data_remaining1:\n",
    "    if len(item['llm_output']) >= 3000:\n",
    "        flag = random.choice([0, 1])\n",
    "        if flag == 1:\n",
    "            output_data_filtered.append(item)\n",
    "        else:\n",
    "            output_data_remaining2.append(item)\n",
    "    else:\n",
    "        output_data_remaining2.append(item)\n",
    "\n",
    "output_data_filtered_path = './processed_outputs/output_data_filtered.json'\n",
    "with open(output_data_filtered_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data_filtered, f, ensure_ascii=False, indent=4)\n",
    "output_data_remaining_path = './processed_outputs/output_data_remaining.json'\n",
    "with open(output_data_remaining_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data_remaining2, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_data_filtered), len(output_data_remaining2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical information of the filtered dataset\n",
    "output_num = 0\n",
    "\n",
    "prompt_types = ['0_shot_cot_restrict', '0_shot_cot', '0_shot_restrict', '0_shot', '5_shot_cot_restrict', '5_shot_cot', '5_shot_restrict', '5_shot']\n",
    "prompt_type_count = {i: 0 for i in prompt_types}\n",
    "\n",
    "models = ['GPT_4o', 'LLaMA3.1-8B', 'DeepSeek-R1-Distill-Llama-8B', 'DeepSeek-R1-Distill-Qwen-1.5B', 'DeepSeek-R1-Distill-Qwen-7B', 'DeepSeek-R1-Distill-Qwen-14B', 'Qwen2-7B-Instruct', 'Qwen2-1.5B-Instruct', 'Qwen2.5-7B-Instruct', 'Qwen2.5-14B-Instruct', 'InternLM2.5-7B-Chat', 'ChatGLM3-6B', 'GLM-4-9B-Chat', 'Gemma-2-9B-it', 'Gemma-2-2B-it', \"Llama-3.2-1B-Instruct\", \"Llama-3.2-3B-Instruct\", \"QwQ-32B-2\", \"Phi-4\"]\n",
    "model_count = {i: 0 for i in models}\n",
    "\n",
    "model_output_avg_length = {i: 0 for i in models}\n",
    "\n",
    "datasets = ['MMLU-Redux_enh', 'MMLU-Redux', 'MMLU-Pro_enh', 'MMLU-Pro', 'DROP', 'GPQA_enh', 'GPQA', 'C-SimpleQA', 'FRAMES', 'AIME_2024', 'LiveMathBench_zh', 'LiveMathBench_en', 'CLUEWSC', 'C-Eval_enh', 'C-Eval', 'SimpleQA', 'AMC23', 'OlympiadBench_en', 'OlympiadBench_zh', 'MGSM', 'CMATH', 'GSM8K', 'MATH', 'AgNews', 'Amazon', 'CMNLI', 'ARC', 'CHID', 'CMMLU_enh', 'CMMLU', 'MMLU_enh', 'MMLU']\n",
    "dataset_count = {i: 0 for i in datasets}\n",
    "\n",
    "key_answer_type_count = {\n",
    "    \"alphabet_option\": 0,\n",
    "    \"math\": 0,\n",
    "    \"short_text\": 0,\n",
    "    \"categorical_label\": 0\n",
    "}\n",
    "output_len_ls = []\n",
    "\n",
    "for item in output_data_filtered:\n",
    "    prompt_type_count[item['setting']] += 1\n",
    "    model_count[item['model_name']] += 1\n",
    "    output_length = len(item['llm_output'])\n",
    "    model_output_avg_length[item['model_name']] += output_length\n",
    "    output_len_ls.append(output_length)\n",
    "    dataset_count[item['dataset']] += 1\n",
    "    key_answer_type_count[item['key_answer_type']] += 1\n",
    "\n",
    "for model in models:\n",
    "    model_output_avg_length[model] = model_output_avg_length[model] / model_count[model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_answer_type_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \" length < 1000: \", len([i for i in output_len_ls if i < 1000]), '\\n',\n",
    "    \"1000 <= length < 2000: \", len([i for i in output_len_ls if 1000 <= i < 2000]), '\\n',\n",
    "    \"2000 <= length < 3000: \", len([i for i in output_len_ls if 2000 <= i < 3000]), '\\n',\n",
    "    \"3000 <= length < 4000: \", len([i for i in output_len_ls if 3000 <= i < 4000]), '\\n',\n",
    "    \"4000 <= length < 5000: \", len([i for i in output_len_ls if 4000 <= i < 5000]), '\\n',\n",
    "    \"5000 <= length < 6000: \", len([i for i in output_len_ls if 5000 <= i < 6000]), '\\n',\n",
    "    \"6000 <= length: \", len([i for i in output_len_ls if 6000 <= i]), '\\n',\n",
    "    \"avg length:\", round(sum(output_len_ls) / len(output_len_ls), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the consistency of two GPT-4o annotations.\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_judgement(text):\n",
    "    if type(text) != str:\n",
    "        print('None: ', text)\n",
    "        return \"None\"\n",
    "    match = re.search(r'\"judgement\": \"(Incorrect|Correct)\"', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        print('Not match: ', text)\n",
    "        return \"None\"\n",
    "\n",
    "\n",
    "def extract(s):\n",
    "    try:\n",
    "        s = s[s.find('{'):s.rfind('}') + 1]\n",
    "        s = json.loads(s)['judgement']\n",
    "    except Exception as e:\n",
    "        s = s.split('\"judgement\": ')[-1]\n",
    "        s = s[s.find('\"') + 1:s.rfind('\"')]\n",
    "    \n",
    "    return s\n",
    "\n",
    "# Two GPT-4o annotation result files\n",
    "judge_result_path1 = \"\"\n",
    "judge_result_path2 = \"\"\n",
    "data_path = \"./processed_outputs/output_data_filtered.json\"\n",
    "\n",
    "with open(judge_result_path1, 'r') as f:\n",
    "    judge_result1 = json.load(f)\n",
    "\n",
    "with open(judge_result_path2, 'r') as f:\n",
    "    judge_result2 = json.load(f)\n",
    "\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(judge_result1['results']), \n",
    "      len(judge_result2['results']), \n",
    "      len(data))\n",
    "num = len(judge_result1['results'])\n",
    "not_same_num = 0\n",
    "for i in range(num):\n",
    "    item1 = judge_result1['results'][i]\n",
    "    item2 = judge_result2['results'][i]\n",
    "    assert item1['llm_output'] == item2['llm_output']\n",
    "    assert item1['llm_output'] == data[i]['llm_output']\n",
    "    judge1 = extract_judgement(item1['GPT_4o_judgment_result'])\n",
    "    judge2 = extract_judgement(item2['GPT_4o_judgment_result'])\n",
    "    data[i]['GPT_4o_judgment_result1'] = judge1\n",
    "    data[i]['GPT_4o_judgment_result2'] = judge2\n",
    "    data[i]['GPT_4o_judgment_consistency'] = \"True\"\n",
    "    data[i]['GPT_4o_final_judgment_result'] = judge1\n",
    "    if judge1 != judge2:\n",
    "        not_same_num += 1\n",
    "        data[i]['GPT_4o_judgment_consistency'] = \"False\"\n",
    "        data[i]['GPT_4o_final_judgment_result'] = \"\"\n",
    "\n",
    "# print(\"Inconsistent sample number: \", not_same_num)\n",
    "\n",
    "output_path = './processed_outputs/output_data_filtered_judge.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples where GPT-4o's judgments are inconsistent across two runs, as well as all math problem samples, for subsequent manual annotation.\n",
    "import json\n",
    "\n",
    "\n",
    "data_path = './processed_outputs/output_data_filtered_judge.json'\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "human_label_data = []\n",
    "\n",
    "for item in data:\n",
    "    if item['GPT_4o_judgment_consistency'] == \"False\" or item['key_answer_type'] == \"math\":\n",
    "        human_label_data.append(item)\n",
    "\n",
    "print(len(human_label_data))\n",
    "output_path = './processed_outputs/output_data_filtered_judge_need_human_label.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(human_label_data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
