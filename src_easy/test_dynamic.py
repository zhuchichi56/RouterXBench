#!/usr/bin/env python3
"""
æµ‹è¯•åŠ¨æ€èåˆprobeçš„ç®€å•è„šæœ¬
åªæµ‹è¯•æ–°çš„probeæ–¹æ³•æ€§èƒ½ï¼Œä¿ç•™åŸæ¥çš„æµ‹è¯•æ¡†æ¶
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from dynamic_probe import run_dynamic_probe_pipeline, DynamicFusionProbe, DynamicProbeDataset
from pathlib import Path
import json
import torch
from torch.utils.data import DataLoader
import numpy as np
import argparse
from typing import List, Optional



def evaluate_uncertainty(model_path: str, test_data, num_samples: int = 100):
    """è¯„ä¼°Dirichletæ¨¡å‹çš„ä¸ç¡®å®šæ€§"""

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)
    metadata = checkpoint['metadata']
    probe_type = metadata.get('probe_type', 'softmax')

    if probe_type != "dirichlet":
        print("Uncertainty evaluation is only available for Dirichlet probes")
        return None

    model = DynamicFusionProbe(
        metadata['input_dim'],
        metadata['num_layers'],
        metadata['output_dim'],
        probe_type=probe_type,
        mlp_hidden_dims=metadata.get('mlp_hidden_dims', None),
        dropout=metadata.get('dropout', 0.1)
    ).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_dataset = DynamicProbeDataset(test_data)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    all_uncertainties = []
    all_predictions = []
    all_labels = []

    print(f"Evaluating uncertainty with {num_samples} samples per prediction...")

    with torch.no_grad():
        for batch_features, batch_labels in test_loader:
            batch_features = batch_features.to(device)
            batch_size = batch_features.size(0)

            # å¤šæ¬¡é‡‡æ ·è®¡ç®—ä¸ç¡®å®šæ€§
            batch_predictions = []
            batch_uncertainties = []

            for _ in range(num_samples):
                model.train()  # å¼€å¯è®­ç»ƒæ¨¡å¼è¿›è¡Œé‡‡æ ·
                logits, uncertainty = model(batch_features, return_uncertainty=True)
                predictions = torch.sigmoid(logits).squeeze(-1)

                batch_predictions.append(predictions.cpu().numpy())
                batch_uncertainties.append(uncertainty.cpu().numpy())

            # è®¡ç®—é¢„æµ‹çš„æ–¹å·®ä½œä¸ºè®¤çŸ¥ä¸ç¡®å®šæ€§
            batch_predictions = np.array(batch_predictions)  # [num_samples, batch_size]
            prediction_variance = np.var(batch_predictions, axis=0)  # [batch_size]
            prediction_mean = np.mean(batch_predictions, axis=0)  # [batch_size]

            # å¹³å‡çš„ç†µä½œä¸ºæ€»ä½“ä¸ç¡®å®šæ€§
            batch_uncertainties = np.array(batch_uncertainties)  # [num_samples, batch_size]
            avg_uncertainty = np.mean(batch_uncertainties, axis=0)  # [batch_size]

            all_predictions.extend(prediction_mean)
            all_uncertainties.extend(avg_uncertainty)
            all_labels.extend(batch_labels.numpy())

    all_predictions = np.array(all_predictions)
    all_uncertainties = np.array(all_uncertainties)
    all_labels = np.array(all_labels)

    # è®¡ç®—ä¸ç¡®å®šæ€§æŒ‡æ ‡
    binary_predictions = (all_predictions > 0.5).astype(int)
    correct_mask = (binary_predictions == all_labels)

    # æ­£ç¡®å’Œé”™è¯¯é¢„æµ‹çš„ä¸ç¡®å®šæ€§åˆ†å¸ƒ
    correct_uncertainty = all_uncertainties[correct_mask]
    incorrect_uncertainty = all_uncertainties[~correct_mask]

    uncertainty_stats = {
        'mean_uncertainty': float(np.mean(all_uncertainties)),
        'std_uncertainty': float(np.std(all_uncertainties)),
        'correct_mean_uncertainty': float(np.mean(correct_uncertainty)) if len(correct_uncertainty) > 0 else 0.0,
        'incorrect_mean_uncertainty': float(np.mean(incorrect_uncertainty)) if len(incorrect_uncertainty) > 0 else 0.0,
        'uncertainty_accuracy_correlation': float(np.corrcoef(all_uncertainties, correct_mask.astype(float))[0, 1])
    }

    print(f"ğŸ” Uncertainty Analysis:")
    print(f"   Mean uncertainty: {uncertainty_stats['mean_uncertainty']:.4f}")
    print(f"   Uncertainty std: {uncertainty_stats['std_uncertainty']:.4f}")
    print(f"   Correct predictions uncertainty: {uncertainty_stats['correct_mean_uncertainty']:.4f}")
    print(f"   Incorrect predictions uncertainty: {uncertainty_stats['incorrect_mean_uncertainty']:.4f}")
    print(f"   Uncertainty-accuracy correlation: {uncertainty_stats['uncertainty_accuracy_correlation']:.4f}")

    return uncertainty_stats

def test_mixed_datasets(test_configs, probe_type, mlp_hidden_dims: List[int] = None,
                       dropout: float = 0.1, save_dir: str = None):
    """æµ‹è¯•æ··åˆæ•°æ®é›†çš„å‡½æ•°"""
    print(f"ğŸŒŸ Training on MIXED datasets:")
    
    try:
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_data = []
        dataset_info = []
        
        print(f"ğŸ”„ Loading {len(test_configs)} datasets...")
        
        for config in test_configs:
            task = config["task"]
            hidden_states_file = config["hidden_states_file"]
            
            if not os.path.exists(hidden_states_file):
                print(f"âŒ File not found: {hidden_states_file}")
                continue
            
            print(f"   ğŸ“ Loading {task}...")
            data = torch.load(hidden_states_file, map_location="cpu", weights_only=False)
            all_data.extend(data)
            dataset_info.append(f"{task}: {len(data)} samples")
            print(f"     âœ… Loaded {len(data)} samples")
        
        if not all_data:
            print("âŒ No valid data loaded!")
            return None
        
        print(f"\nğŸ”— Combined total: {len(all_data)} samples")
        
        # åˆ›å»ºæ··åˆä»»åŠ¡å
        mixed_task_name = "mixed_" + "_".join([config["task"] for config in test_configs])
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æ··åˆæ•°æ®
        mixed_file = f"../temp_mixed_data_{probe_type}.pt"
        os.makedirs(os.path.dirname(os.path.abspath(mixed_file)), exist_ok=True)
        torch.save(all_data, mixed_file)
        
        print(f"ğŸ’¾ Saved mixed data to: {mixed_file}")
        print(f"ğŸš€ Training {probe_type} probe on mixed dataset...")

        # ä½¿ç”¨ç°æœ‰çš„æµ‹è¯•å‡½æ•°
        results = test_dynamic_probe_on_task(mixed_task_name, mixed_file, probe_type,
                                            mlp_hidden_dims=mlp_hidden_dims, dropout=dropout,
                                            save_dir=save_dir)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(mixed_file)
            print(f"ğŸ—‘ï¸ Cleaned up temporary file")
        except:
            pass

        if results:
            # æ·»åŠ æ•°æ®é›†ä¿¡æ¯åˆ°ç»“æœä¸­
            results['dataset_info'] = dataset_info
            results['total_samples'] = len(all_data)
            print(f"âœ… Mixed training completed successfully!")
        
        return results
        
    except Exception as e:
        print(f"âŒ Mixed dataset training failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def test_dynamic_probe_on_task(task: str, hidden_states_file: str, probe_type: str = "softmax",
                              mlp_hidden_dims: List[int] = None, dropout: float = 0.1,
                              save_dir: str = None):
    """æµ‹è¯•åŠ¨æ€probeåœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½"""

    print(f"=" * 60)
    print(f"Testing Dynamic Fusion Probe ({probe_type}) on task: {task}")
    print(f"=" * 60)

    if not os.path.exists(hidden_states_file):
        print(f"âŒ Hidden states file not found: {hidden_states_file}")
        return None

    try:
        # è¿è¡ŒåŠ¨æ€probeè®­ç»ƒå’Œæµ‹è¯•
        results = run_dynamic_probe_pipeline(
            task=task,
            hidden_states_file=hidden_states_file,
            save_dir=save_dir or "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/src/probe_save/test",
            probe_type=probe_type,
            mlp_hidden_dims=mlp_hidden_dims,
            dropout=dropout
        )

        # è¾“å‡ºç»“æœ
        training_results = results['training_results']
        test_results = results['test_results']

        print(f"\nğŸ“Š Training Results:")
        print(f"   Best validation loss: {training_results['best_val_loss']:.4f}")
        print(f"   Final layer weights: {training_results['final_layer_weights']}")

        if probe_type == "dirichlet":
            print(f"   Final concentration: {training_results['final_concentration']}")
            print(f"   Global concentration (Î²â‚€): {training_results['final_global_concentration']:.4f}")

        print(f"\nğŸ“Š Test Results:")
        print(f"   Test accuracy: {test_results['accuracy']:.4f}")

        if probe_type == "dirichlet":
            print(f"   Final concentration: {test_results['concentration']}")
            print(f"   Global concentration (Î²â‚€): {test_results['global_concentration']:.4f}")

        print(f"\nğŸ’¾ Model saved to: {results['model_path']}")

        # ä¸ºDirichletæ¨¡å‹è¯„ä¼°ä¸ç¡®å®šæ€§
        if probe_type == "dirichlet":
            print(f"\nğŸ” Evaluating uncertainty for Dirichlet model...")
            # åŠ è½½æµ‹è¯•æ•°æ®è¿›è¡Œä¸ç¡®å®šæ€§è¯„ä¼°
            data = torch.load(hidden_states_file, map_location="cpu",weights_only= False)
            split_idx = int(len(data) * 0.8)
            val_data = data[split_idx:]  # ä½¿ç”¨éªŒè¯é›†è¯„ä¼°ä¸ç¡®å®šæ€§

            uncertainty_stats = evaluate_uncertainty(results['model_path'], val_data, num_samples=50)
            if uncertainty_stats:
                results['uncertainty_stats'] = uncertainty_stats

        return results

    except Exception as e:
        print(f"âŒ Error during testing: {e}")
        return None

def test_mixed_datasets_with_sampling(test_configs, probe_type, max_samples=None,
                                     mlp_hidden_dims: List[int] = None, dropout: float = 0.1,
                                     save_dir: str = None):
    """æµ‹è¯•æ··åˆæ•°æ®é›†çš„å‡½æ•°ï¼Œæ”¯æŒé‡‡æ ·é™åˆ¶"""
    print(f"ğŸŒŸ Training on MIXED datasets (max_samples: {max_samples}):")
    
    try:
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_data = []
        dataset_info = []
        
        print(f"ğŸ”„ Loading {len(test_configs)} datasets...")
        
        for config in test_configs:
            task = config["task"]
            hidden_states_file = config["hidden_states_file"]
            
            if not os.path.exists(hidden_states_file):
                print(f"âŒ File not found: {hidden_states_file}")
                continue
            
            print(f"   ğŸ“ Loading {task}...")
            data = torch.load(hidden_states_file, map_location="cpu", weights_only=False)
            all_data.extend(data)
            dataset_info.append(f"{task}: {len(data)} samples")
            print(f"     âœ… Loaded {len(data)} samples")
        
        if not all_data:
            print("âŒ No valid data loaded!")
            return None
        
        # å¦‚æœæŒ‡å®šäº†max_samplesï¼Œè¿›è¡Œéšæœºé‡‡æ ·
        if max_samples and len(all_data) > max_samples:
            print(f"ğŸ¯ Sampling {max_samples} from {len(all_data)} total samples")
            import random
            random.shuffle(all_data)
            all_data = all_data[:max_samples]
        
        print(f"\nğŸ”— Final dataset size: {len(all_data)} samples")
        
        # åˆ›å»ºæ··åˆä»»åŠ¡åï¼ŒåŒ…å«æ ·æœ¬æ•°ä¿¡æ¯
        sample_suffix = f"_{max_samples}samples" if max_samples else "_allsamples"
        mixed_task_name = "mixed_" + "_".join([config["task"] for config in test_configs]) 
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æ··åˆæ•°æ®
        mixed_file = f"../temp_mixed_data_{probe_type}{sample_suffix}.pt"
        os.makedirs(os.path.dirname(os.path.abspath(mixed_file)), exist_ok=True)
        torch.save(all_data, mixed_file)
        
        print(f"ğŸ’¾ Saved mixed data to: {mixed_file}")
        print(f"ğŸš€ Training {probe_type} probe on mixed dataset...")

        # ä½¿ç”¨ç°æœ‰çš„æµ‹è¯•å‡½æ•°
        results = test_dynamic_probe_on_task(mixed_task_name, mixed_file, probe_type,
                                            mlp_hidden_dims=mlp_hidden_dims, dropout=dropout,
                                            save_dir=save_dir)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(mixed_file)
            print(f"ğŸ—‘ï¸ Cleaned up temporary file")
        except:
            pass

        if results:
            # æ·»åŠ æ•°æ®é›†ä¿¡æ¯åˆ°ç»“æœä¸­
            results['dataset_info'] = dataset_info
            results['total_samples'] = len(all_data)
            results['max_samples'] = max_samples
            print(f"âœ… Mixed training completed successfully!")
        
        return results
        
    except Exception as e:
        print(f"âŒ Mixed dataset training failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def main_with_sampling(datasets=None, probe_types=None, max_samples=None,
                      mlp_hidden_dims: List[int] = None, dropout: float = 0.1,
                      save_dir: str = None):
    """ä¸»æµ‹è¯•å‡½æ•° - æ”¯æŒä¸åŒé‡‡æ ·å¤§å°

    Args:
        datasets: æ•°æ®é›†åç§°åˆ—è¡¨ï¼Œå¦‚ ["alpaca_5k", "mmlu_train", "big_math"]
        probe_types: probeç±»å‹åˆ—è¡¨ï¼Œå¦‚ ["softmax", "dirichlet"]
        max_samples: æœ€å¤§é‡‡æ ·æ•°ï¼Œå¦‚ 12000
        mlp_hidden_dims: MLPéšè—å±‚ç»´åº¦åˆ—è¡¨ï¼Œå¦‚ [64, 128]
        dropout: Dropoutæ¯”ç‡
        save_dir: ä¿å­˜ç›®å½•è·¯å¾„
    """

    # æ•°æ®é›†æ˜ å°„è¡¨
    dataset_map = {
        "alpaca_5k": {
            "task": "alpaca_5k_train",
            "hidden_states_file": "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/hs/Llama-3.1-8B-Instruct_alpaca_5k_train.pt"
        },
        "mmlu_train": {
            "task": "mmlu_train",
            "hidden_states_file": "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/hs/Llama-3.1-8B-Instruct_mmlu_train.pt"
        },
        "big_math": {
            "task": "big_math_5k_train",
            "hidden_states_file": "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/hs/Llama-3.1-8B-Instruct_big_math_5k_train.pt"
        }
    }

    # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–é»˜è®¤å€¼
    if datasets is None:
        datasets = ["alpaca_5k", "mmlu_train", "big_math"]

    if probe_types is None:
        probe_types = ["softmax", "dirichlet"]

    # æ ¹æ®æ•°æ®é›†åç§°æ„å»ºtest_configs
    test_configs = []
    for dataset_name in datasets:
        if dataset_name in dataset_map:
            test_configs.append(dataset_map[dataset_name])
        else:
            print(f"âš ï¸  Unknown dataset: {dataset_name}, skipping...")

    if not test_configs:
        print("âŒ No valid datasets specified!")
        return

    # å®šä¹‰ä¸åŒçš„é‡‡æ ·å¤§å°
    sample_sizes = [max_samples] if max_samples else [None]  # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
    
    all_results = {}

    for probe_type in probe_types:
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ Testing {probe_type.upper()} probe type with different sample sizes")
        print(f"{'='*80}")
        
        all_results[probe_type] = {}
        
        for sample_size in sample_sizes:
            size_label = f"{sample_size}samples" if sample_size else "allsamples"
            print(f"\nğŸ¯ Training with sample size: {size_label}")
            print(f"{'-'*60}")

            # æµ‹è¯•æ··åˆæ•°æ®é›†
            mixed_results = test_mixed_datasets_with_sampling(test_configs, probe_type, sample_size,
                                                             mlp_hidden_dims=mlp_hidden_dims,
                                                             dropout=dropout, save_dir=save_dir)
            
            if mixed_results:
                mixed_summary = {
                    "accuracy": mixed_results['test_results']['accuracy'],
                    "best_val_loss": mixed_results['training_results']['best_val_loss'],
                    "layer_weights": mixed_results['test_results']['layer_weights'].tolist(),
                    "total_samples": mixed_results['total_samples'],
                    "max_samples": mixed_results['max_samples']
                }
                
                if probe_type == "dirichlet":
                    mixed_summary.update({
                        "concentration": mixed_results['test_results']['concentration'].tolist(),
                        "global_concentration": mixed_results['test_results']['global_concentration']
                    })
                    
                    if 'uncertainty_stats' in mixed_results:
                        mixed_summary['uncertainty_stats'] = mixed_results['uncertainty_stats']
                
                # ä½¿ç”¨æ ·æœ¬å¤§å°ä½œä¸ºé”®
                all_results[probe_type][f"mixed_{size_label}"] = mixed_summary
                
                print(f"âœ… Mixed training completed - Accuracy: {mixed_results['test_results']['accuracy']:.4f}")
                print(f"   Sample size: {mixed_results['total_samples']}")
                print(f"   Best val loss: {mixed_results['training_results']['best_val_loss']:.4f}")
                
                if probe_type == "dirichlet":
                    print(f"   Global concentration: {mixed_results['test_results']['global_concentration']:.4f}")
                    if 'uncertainty_stats' in mixed_results:
                        print(f"   Mean uncertainty: {mixed_results['uncertainty_stats']['mean_uncertainty']:.4f}")
            
            print(f"\n{'-'*60}")

    # ä¿å­˜æ‰€æœ‰æµ‹è¯•ç»“æœ
    if all_results:
       

        # æ‰“å°æ€»ç»“
        print(f"\nğŸ“‹ Summary of Dynamic Probe Performance by Sample Size:")
        for probe_type, probe_results in all_results.items():
            print(f"\n{probe_type.upper()} Probe:")
            for size_key, result in probe_results.items():
                accuracy = result['accuracy']
                samples = result['total_samples']
                val_loss = result['best_val_loss']
                
                print(f"   {size_key}: Accuracy = {accuracy:.4f}, Samples = {samples}, Val Loss = {val_loss:.4f}")
                
                if probe_type == "dirichlet":
                    global_conc = result['global_concentration']
                    print(f"       Global concentration (Î²â‚€) = {global_conc:.4f}")
                    
                    if 'uncertainty_stats' in result:
                        mean_unc = result['uncertainty_stats']['mean_uncertainty']
                        print(f"       Mean uncertainty = {mean_unc:.4f}")

        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨
        print(f"\nğŸ“Š Performance Comparison Table:")
        print(f"{'Probe Type':<12} {'Sample Size':<15} {'Accuracy':<10} {'Val Loss':<10} {'Samples':<8}")
        print(f"{'-'*65}")
        
        for probe_type in probe_types:
            for size_key, result in all_results[probe_type].items():
                accuracy = result['accuracy']
                val_loss = result['best_val_loss']
                samples = result['total_samples']
                
                print(f"{probe_type:<12} {size_key:<15} {accuracy:.4f}     {val_loss:.4f}     {samples:<8}")


def main_single_datasets_with_sampling():
    """è®­ç»ƒå•ä¸ªæ•°æ®é›†çš„ä¸åŒé‡‡æ ·å¤§å°"""
    
    # å®šä¹‰æµ‹è¯•ä»»åŠ¡
    test_configs = [
        {
            "task": "alpaca_5k_train",
            "hidden_states_file": "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/hs/Llama-3.1-8B-Instruct_alpaca_5k_train.pt"
        },
        {
            "task": "mmlu_train", 
            "hidden_states_file": "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/hs/Llama-3.1-8B-Instruct_mmlu_train.pt"
        },
        {
            "task": "big_math_5k_train",
            "hidden_states_file": "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/hs/Llama-3.1-8B-Instruct_big_math_5k_train.pt"
        }
    ]
    
    probe_types = ["softmax", "dirichlet"]
    sample_sizes = [None]
    
    all_results = {}
    
    for probe_type in probe_types:
        all_results[probe_type] = {}
        
        for config in test_configs:
            task = config["task"]
            hidden_states_file = config["hidden_states_file"]
            
            print(f"\nğŸ”¬ Testing {probe_type.upper()} on {task}")
            
            for sample_size in sample_sizes:
                size_label = f"{sample_size}samples" if sample_size else "allsamples"
                
                # åŠ è½½å¹¶é‡‡æ ·æ•°æ®
                if os.path.exists(hidden_states_file):
                    data = torch.load(hidden_states_file, map_location="cpu", weights_only=False)
                    
                    if sample_size and len(data) > sample_size:
                        import random
                        random.shuffle(data)
                        data = data[:sample_size]
                    
                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                    temp_file = f"../temp_{task}_{size_label}.pt"
                    torch.save(data, temp_file)
                    
                    # æµ‹è¯•
                    task_with_size = f"{task}_{size_label}"
                    results = test_dynamic_probe_on_task(task_with_size, temp_file, probe_type)
                    
                    if results:
                        result_key = f"{task}_{size_label}"
                        all_results[probe_type][result_key] = {
                            "accuracy": results['test_results']['accuracy'],
                            "best_val_loss": results['training_results']['best_val_loss'],
                            "total_samples": len(data)
                        }
                        
                        print(f"   {size_label}: Accuracy = {results['test_results']['accuracy']:.4f}")
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.remove(temp_file)
                    except:
                        pass
    
    return all_results



def test_mixed_datasets_for_leave_one_category(test_configs, probe_type, custom_save_name):
    """ä¸ºç•™ä¸€ç±»å®éªŒä¸“é—¨è®¾è®¡çš„æ··åˆæ•°æ®é›†æµ‹è¯•å‡½æ•°"""
    
    print(f"ğŸŒŸ Training mixed datasets for leave-one-category experiment:")
    
    try:
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_data = []
        dataset_info = []
        
        print(f"ğŸ”„ Loading {len(test_configs)} datasets...")
        
        for config in test_configs:
            task = config["task"]
            hidden_states_file = config["hidden_states_file"]
            
            if not os.path.exists(hidden_states_file):
                print(f"âŒ File not found: {hidden_states_file}")
                continue
            
            print(f"   ğŸ“ Loading {task}...")
            data = torch.load(hidden_states_file, map_location="cpu", weights_only=False)
            all_data.extend(data)
            dataset_info.append(f"{task}: {len(data)} samples")
            print(f"     âœ… Loaded {len(data)} samples")
        
        if not all_data:
            print("âŒ No valid data loaded!")
            return None
        
        print(f"\nğŸ”— Combined total: {len(all_data)} samples")
        
        # åˆ›å»ºæ··åˆä»»åŠ¡å
        mixed_task_name = f"mmlu_pro_{custom_save_name}"
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æ··åˆæ•°æ®
        mixed_file = f"../temp_mixed_data_{custom_save_name}_{probe_type}.pt"
        os.makedirs(os.path.dirname(os.path.abspath(mixed_file)), exist_ok=True)
        torch.save(all_data, mixed_file)
        
        print(f"ğŸ’¾ Saved mixed data to: {mixed_file}")
        print(f"ğŸš€ Training {probe_type} probe on mixed dataset...")
        
        # ä½¿ç”¨ä¸“é—¨çš„ä¿å­˜ç›®å½•
        save_dir = "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/src/probe_save/mmlu_pro"
        results = run_dynamic_probe_pipeline(
            task=mixed_task_name,
            hidden_states_file=mixed_file,
            save_dir=save_dir,
            probe_type=probe_type
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(mixed_file)
            print(f"ğŸ—‘ï¸ Cleaned up temporary file")
        except:
            pass
        
        if results:
            # æ·»åŠ æ•°æ®é›†ä¿¡æ¯åˆ°ç»“æœä¸­
            results['dataset_info'] = dataset_info
            results['total_samples'] = len(all_data)
            print(f"âœ… Mixed training completed successfully!")
        
        return results
        
    except Exception as e:
        print(f"âŒ Mixed dataset training failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def main_leave_one_category_mmlu_pro():
    """ä¸‰ç¼ºä¸€ï¼ˆMMLU-Pro æŒ‰å¤§ç±»ç•™ä¸€è®­ç»ƒï¼‰çš„åŠ¨æ€probeå®éªŒ"""
    
    # å®šä¹‰MMLU-Proçš„å››ä¸ªå¤§ç±»
    categories = {
        "stem": ["chemistry", "computer_science", "engineering", "math", "physics"],
        "humanities": ["history", "philosophy", "law"],
        "social_sciences": ["economics", "psychology", "other"],
        "other_disciplines": ["business", "health", "medicine"]
    }
    
    # åŸºç¡€æ–‡ä»¶è·¯å¾„æ¨¡æ¿
    base_path = "/volume/pt-train/users/wzhang/ghchen/zh/CoBench/hs"
    
    probe_types = ["softmax", "dirichlet"]
    
    all_results = {}
    
    print("\n================ LEAVE-ONE-CATEGORY (MMLU-Pro) DYNAMIC PROBE TRAINING ================")
    
    for probe_type in probe_types:
        all_results[probe_type] = {}
        
        print(f"\nğŸ”¬ Testing {probe_type.upper()} probe with leave-one-category strategy")
        
        for leave_out in categories.keys():
            include_categories = [c for c in categories.keys() if c != leave_out]
            
            # æ„å»ºè®­ç»ƒä»»åŠ¡é…ç½®åˆ—è¡¨
            train_configs = []
            total_tasks = 0
            
            for cat in include_categories:
                for subject in categories[cat]:
                    task_name = f"mmlu_pro_{subject}"
                    hidden_states_file = f"{base_path}/mmlu_pro/Llama-3.1-8B-Instruct_{task_name}.pt"
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if os.path.exists(hidden_states_file):
                        train_configs.append({
                            "task": task_name,
                            "hidden_states_file": hidden_states_file
                        })
                        total_tasks += 1
                    else:
                        print(f"âš ï¸  File not found: {hidden_states_file}")
            
            print("\n" + "=" * 80)
            print(f"ğŸš€ Training with three categories, leaving out: {leave_out}")
            print(f"ğŸ“š Using subjects from: {', '.join(include_categories)}")
            print(f"ğŸ§© Total valid tasks: {total_tasks}")
            
            if not train_configs:
                print("âŒ No valid training data found for this configuration!")
                continue
            
            # è®­ç»ƒæ··åˆæ•°æ®é›†
            custom_save_name = f"leaveout_{leave_out.lower()}"
            mixed_results = test_mixed_datasets_for_leave_one_category(
                train_configs, 
                probe_type, 
                custom_save_name
            )
            
            if mixed_results:
                result_key = f"leaveout_{leave_out.lower()}"
                all_results[probe_type][result_key] = {
                    "accuracy": mixed_results['test_results']['accuracy'],
                    "best_val_loss": mixed_results['training_results']['best_val_loss'],
                    "layer_weights": mixed_results['test_results']['layer_weights'].tolist(),
                    "total_samples": mixed_results['total_samples'],
                    "leave_out_category": leave_out,
                    "include_categories": include_categories,
                    "total_tasks": total_tasks
                }
                
                if probe_type == "dirichlet":
                    result_key_data = all_results[probe_type][result_key]
                    result_key_data.update({
                        "concentration": mixed_results['test_results']['concentration'].tolist(),
                        "global_concentration": mixed_results['test_results']['global_concentration']
                    })
                    
                    if 'uncertainty_stats' in mixed_results:
                        result_key_data['uncertainty_stats'] = mixed_results['uncertainty_stats']
                
                print(f"âœ… Leave-one-category training completed - Accuracy: {mixed_results['test_results']['accuracy']:.4f}")
                print(f"   Left out: {leave_out}")
                print(f"   Sample size: {mixed_results['total_samples']}")
                print(f"   Best val loss: {mixed_results['training_results']['best_val_loss']:.4f}")
                
                if probe_type == "dirichlet":
                    print(f"   Global concentration: {mixed_results['test_results']['global_concentration']:.4f}")
                    if 'uncertainty_stats' in mixed_results:
                        print(f"   Mean uncertainty: {mixed_results['uncertainty_stats']['mean_uncertainty']:.4f}")
            else:
                print(f"âŒ Failed to train for leave-out category: {leave_out}")
    
    # ä¿å­˜ç»“æœ
    if all_results:
        results_file = "../results/dynamic_probe_leave_one_category_results.json"
        os.makedirs(os.path.dirname(results_file), exist_ok=True)
        
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2)
        
        print(f"\nğŸ“„ Leave-one-category results saved to: {results_file}")
        
        # æ‰“å°æ€»ç»“
        print(f"\nğŸ“‹ Summary of Leave-One-Category Dynamic Probe Performance:")
        for probe_type, probe_results in all_results.items():
            print(f"\n{probe_type.upper()} Probe:")
            for result_key, result in probe_results.items():
                accuracy = result['accuracy']
                samples = result['total_samples']
                val_loss = result['best_val_loss']
                leave_out = result['leave_out_category']
                
                print(f"   {result_key}: Accuracy = {accuracy:.4f}, Samples = {samples}, Val Loss = {val_loss:.4f}")
                print(f"       Left out: {leave_out}")
                
                if probe_type == "dirichlet":
                    global_conc = result['global_concentration']
                    print(f"       Global concentration (Î²â‚€) = {global_conc:.4f}")
                    
                    if 'uncertainty_stats' in result:
                        mean_unc = result['uncertainty_stats']['mean_uncertainty']
                        print(f"       Mean uncertainty = {mean_unc:.4f}")
        
        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨
        print(f"\nğŸ“Š Leave-One-Category Performance Comparison:")
        print(f"{'Probe Type':<12} {'Left Out':<18} {'Accuracy':<10} {'Val Loss':<10} {'Samples':<8}")
        print(f"{'-'*70}")
        
        for probe_type in probe_types:
            if probe_type in all_results:
                for result_key, result in all_results[probe_type].items():
                    accuracy = result['accuracy']
                    val_loss = result['best_val_loss']
                    samples = result['total_samples']
                    leave_out = result['leave_out_category']
                    
                    print(f"{probe_type:<12} {leave_out:<18} {accuracy:.4f}     {val_loss:.4f}     {samples:<8}")
    
    print("\nâœ… Completed leave-one-category training for all four choices.")
    return all_results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Dynamic Fusion Probe Training and Testing',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # åŸºæœ¬ç”¨æ³•
  python test_dynamic.py --datasets alpaca_5k mmlu_train --probe_types softmax

  # ä½¿ç”¨ MLP ç»“æ„å’Œè‡ªå®šä¹‰é…ç½®
  python test_dynamic.py \\
    --datasets alpaca_5k mmlu_train big_math \\
    --probe_types softmax dirichlet \\
    --max_samples 12000 \\
    --mlp_hidden_dims 64 128 \\
    --dropout 0.5 \\
    --save_dir custom/save/path

  # å•æ•°æ®é›†è®­ç»ƒ
  python test_dynamic.py --datasets alpaca_5k --probe_types dirichlet --max_samples 5000
        """
    )

    parser.add_argument('--datasets', type=str, nargs='+',
                       default=["alpaca_5k", "mmlu_train", "big_math"],
                       help='æ•°æ®é›†åç§°åˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼Œå¯é€‰: alpaca_5k, mmlu_train, big_math')

    parser.add_argument('--probe_types', type=str, nargs='+',
                       default=["softmax", "dirichlet"],
                       help='Probe ç±»å‹åˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼Œå¯é€‰: softmax, dirichlet')

    parser.add_argument('--max_samples', type=int, default=12000,
                       help='æœ€å¤§é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤: 12000ï¼‰')

    parser.add_argument('--mlp_hidden_dims', type=int, nargs='*', default=None,
                       help='MLP éšè—å±‚ç»´åº¦åˆ—è¡¨ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ï¼Œä¾‹å¦‚: 64 128ã€‚ç•™ç©ºè¡¨ç¤ºå•å±‚çº¿æ€§åˆ†ç±»å™¨')

    parser.add_argument('--dropout', type=float, default=0.1,
                       help='Dropout æ¯”ç‡ï¼ˆé»˜è®¤: 0.1ï¼‰')

    parser.add_argument('--save_dir', type=str,
                       default="/volume/pt-train/users/wzhang/ghchen/zh/CoBench/src/probe_save/",
                       help='æ¨¡å‹å’Œå†å²ä¿å­˜ç›®å½•')

    args = parser.parse_args()

    print(f"ğŸ“‹ Running with parameters:")
    print(f"   Datasets: {args.datasets}")
    print(f"   Probe types: {args.probe_types}")
    print(f"   Max samples: {args.max_samples}")
    print(f"   MLP hidden dims: {args.mlp_hidden_dims}")
    print(f"   Dropout: {args.dropout}")
    print(f"   Save directory: {args.save_dir}")
    print()

    # è¿è¡Œæ··åˆæ•°æ®é›†çš„é‡‡æ ·å®éªŒ
    main_with_sampling(
        datasets=args.datasets,
        probe_types=args.probe_types,
        max_samples=args.max_samples,
        mlp_hidden_dims=args.mlp_hidden_dims,
        dropout=args.dropout,
        save_dir=args.save_dir
    )

    # main_leave_one_category_mmlu_pro()

    # main_single_datasets_with_sampling()
 
    